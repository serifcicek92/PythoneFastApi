##############################################################################
#train_mistral7b
#eƒüitmek i√ßin gpu ve ram yetmedi 
##############################################################################
##############################################################################
##############################################################################
#C:\huggingface\generated\mistral7b>python d:\Projeler\PythoneFastApi\train_mistral7b.py
#pip install torch accelerate transformers peft datasets
#ollama create mistral-7b-finetuned --model-path ./mistral-7b-finetuned

#pip install torch accelerate transformers peft datasets
#ollama create mistral-7b-finetuned --model-path ./mistral-7b-finetuned
############################################
# -*- coding: utf-8 -*-
"""
VRAM dostu Mistral 7B + LoRA fine-tune
RTX 2060 (6GB) i√ßin optimize edildi
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, TaskType

# -------------------
# 1Ô∏è‚É£ Ayarlar
# -------------------
base_model = "mistralai/Mistral-7B-v0.1"
dataset_path = r"D:\Projeler\PythoneFastApi\kural.jsonl"
output_dir = r"C:\huggingface\mistral-7b-finetuned"
epochs = 3
per_device_batch = 1
grad_accum = 8       # efektif batch 8
lr = 3e-4
max_length = 512

gpu_available = torch.cuda.is_available()
print("GPU mevcut mu?", gpu_available)
if gpu_available:
    print("GPU adƒ±:", torch.cuda.get_device_name(0))
    print("VRAM:", torch.cuda.get_device_properties(0).total_memory / 1024**3, "GB")

# -------------------
# 2Ô∏è‚É£ Tokenizer ve Model
# -------------------
tokenizer = AutoTokenizer.from_pretrained(base_model)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# CPU offload ile y√ºkleme
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    device_map="auto",        # HuggingFace Accelerate ile CPU+GPU dengeli
    torch_dtype=torch.float32,  # FP16 kapalƒ± (RTX 2060 i√ßin)
    low_cpu_mem_usage=True
)

# -------------------
# 2Ô∏è‚É£a LoRA Config
# -------------------
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # sadece dikkat katmanlarƒ±
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# -------------------
# 3Ô∏è‚É£ Dataset ve Tokenize
# -------------------
dataset = load_dataset("json", data_files=dataset_path, split="train")

def tokenize_fn(example):
    text = f"{example['prompt']}\n{example['completion']}"
    tokenized = tokenizer(
        text,
        truncation=True,
        max_length=max_length,
        padding="max_length"
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_fn, remove_columns=dataset.column_names)

# -------------------
# 4Ô∏è‚É£ Data Collator
# -------------------
data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)

# -------------------
# 5Ô∏è‚É£ TrainingArguments
# -------------------
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_batch,
    gradient_accumulation_steps=grad_accum,
    num_train_epochs=epochs,
    learning_rate=lr,
    fp16=False,                     # FP16 kapalƒ±
    logging_strategy="steps",
    logging_steps=5,                # anlƒ±k ilerlemeyi g√∂rmek i√ßin
    save_strategy="epoch",
    save_total_limit=2,
    optim="adamw_torch",           # 8-bit optimizer kaldƒ±rƒ±ldƒ±
    report_to="none",
    remove_unused_columns=False,
    dataloader_num_workers=0,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# -------------------
# 6Ô∏è‚É£ Eƒüitim
# -------------------
print("üöÄ Eƒüitim ba≈ülƒ±yor...")
trainer.train()

# -------------------
# 7Ô∏è‚É£ Kaydet
# -------------------
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"\n‚úÖ Fine-tune tamamlandƒ±! Model '{output_dir}' klas√∂r√ºne kaydedildi.")





##############################################################################
##############################################################################
##############################################################################
#chat test 

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_path = "C:/huggingface/flan-t5-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

while True:
    prompt = input("\nüß† Soru: ")
    if prompt.lower() in ["q", "quit", "exit"]:
        break
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üí¨ Cevap: {response}")

##############################################################################
##############################################################################
##############################################################################
#flan-t5-base

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments
from datasets import load_dataset

# ===== GPU Bilgisi =====
if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    vram_gb = round(torch.cuda.get_device_properties(0).total_memory / (1024 ** 3), 2)
    print(f"‚úÖ GPU aktif: {gpu_name}\nVRAM (GB): {vram_gb}")
else:
    print("‚ùå GPU bulunamadƒ±, CPU kullanƒ±lacak.")
device = "cuda" if torch.cuda.is_available() else "cpu"

# ===== Model ve Dataset Ayarlarƒ± =====
base_model = "google/flan-t5-base"
dataset_path = r"D:\Projeler\PythoneFastApi\kural.jsonl"
output_dir = r"C:\huggingface\flan-t5-finetuned"

print("\nüì• Model y√ºkleniyor...")
model = AutoModelForSeq2SeqLM.from_pretrained(base_model, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
tokenizer = AutoTokenizer.from_pretrained(base_model)

print("üìÇ Dataset y√ºkleniyor...")
dataset = load_dataset("json", data_files={"train": dataset_path})

# ===== Tokenization Fonksiyonu =====
def tokenize(examples):
    inputs = examples["prompt"]
    outputs = examples["completion"]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True)
    labels = tokenizer(outputs, max_length=512, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

print("‚úèÔ∏è Dataset tokenize ediliyor...")
tokenized_dataset = dataset["train"].map(tokenize, batched=True)

# ===== Eƒüitim Ayarlarƒ± =====
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=3e-4,
    fp16=False,#torch.cuda.is_available(),
    bf16=True,
    logging_steps=1,
    save_steps=200,
    save_total_limit=2,
    report_to="none"
)

# ===== Trainer =====
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

# ===== Eƒüitim =====
print("üöÄ Eƒüitim ba≈ülƒ±yor...")
trainer.train()

# ===== Kaydet =====
print("\n‚úÖ Eƒüitim tamamlandƒ±, model kaydediliyor...")
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"üéâ Model kaydedildi: {output_dir}")

##############################################################################
##############################################################################
##############################################################################
#google/flan-t5-base tetikleme kƒ±smƒ± promptest
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ================================
# 1Ô∏è‚É£ Orijinal model ve tokenizer
# ================================
base_model = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSeq2SeqLM.from_pretrained(base_model)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print(f"‚úÖ {base_model} modeli y√ºklendi ({device})")

# ================================
# 2Ô∏è‚É£ Sohbet fonksiyonu
# ================================
def chat(prompt, max_new_tokens=256):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2
    )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# ================================
# 3Ô∏è‚É£ Sohbet d√∂ng√ºs√º
# ================================
print("\nüí¨ FLAN-T5 Base ile sohbet! (√ßƒ±kmak i√ßin 'q' yaz)")
while True:
    user_input = input("\nüß† Sen: ")
    if user_input.lower() in ["q", "quit", "exit"]:
        print("üëã G√∂r√º≈ü√ºr√ºz!")
        break
    response = chat(user_input)
    print(f"ü§ñ Model: {response}")

##############################################################################
##############################################################################
##############################################################################
#flan-t5-base part 2 DOƒûRU √áALI≈ûTI AMA ƒ∞NGƒ∞Lƒ∞ZCEDEN DOLAYI SIKINTI
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset

# Ayarlar
base_model = "google/flan-t5-base"  # K√º√ß√ºk model, 6GB GPU i√ßin uygun
dataset_path = "D:/Projeler/PythoneFastApi/kural.jsonl"
output_dir = "C:/huggingface/flan-t5-finetuned"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ GPU aktif: {device}" if device=="cuda" else "‚úÖ CPU kullanƒ±lacak")

# Model ve tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSeq2SeqLM.from_pretrained(base_model).to(device)

# Pad token ayarƒ±
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Dataset y√ºkleme
dataset = load_dataset("json", data_files=dataset_path)

# Tokenizasyon fonksiyonu
def tokenize(examples):
    inputs = examples["prompt"]
    targets = examples["completion"]
    model_inputs = tokenizer(inputs, max_length=128, padding="max_length", truncation=True)
    labels = tokenizer(targets, max_length=256, padding="max_length", truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(tokenize, batched=True)

# Training args
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=3e-4,
    num_train_epochs=3,
    logging_steps=5,
    save_steps=10,
    fp16=torch.cuda.is_available(),  # GPU varsa fp16 kullan
    save_total_limit=2,
    predict_with_generate=True
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"] if "train" in tokenized_dataset else tokenized_dataset,
    tokenizer=tokenizer
)

# Eƒüitim ba≈ülat
trainer.train()

# Modeli kaydet
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"‚úÖ Eƒüitim tamamlandƒ±, model kaydedildi: {output_dir}")


##############################################################################
##############################################################################
##############################################################################
#PROMT TEST 
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_path = "C:/huggingface/flan-t5-finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

while True:
    prompt = input("\nüß† Soru: ")
    if prompt.lower() in ["q", "quit", "exit"]:
        break
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üí¨ Cevap: {response}")
##############################################################################
##############################################################################
##############################################################################
#SambaLingo-Turkish-Chat ram yetersiz kaldƒ± eƒüitim i√ßin
    import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import load_dataset

# Ayarlar
base_model = "sambanovasystems/SambaLingo-Turkish-Chat"
dataset_path = "D:/Projeler/PythoneFastApi/kural.jsonl"
output_dir = "C:/huggingface/SambaLingo-Turkish-Chat-Finetuned"
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ GPU aktif: {device}" if device=="cuda" else "‚úÖ CPU kullanƒ±lacak")

# =============================
# Model ve tokenizer
# =============================
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model).to(device)

# Pad token ayarƒ± (Causal LM i√ßin genellikle eos_token kullanƒ±lƒ±r)
tokenizer.pad_token = tokenizer.eos_token

# =============================
# Dataset y√ºkleme
# =============================
dataset = load_dataset("json", data_files=dataset_path)

# =============================
# Tokenizasyon fonksiyonu
# =============================
def tokenize(examples):
    # Prompt ve completion ayrƒ± tutuluyor
    tokenized_inputs = tokenizer(
        examples["prompt"],
        max_length=256,
        padding="max_length",
        truncation=True
    )
    labels = tokenizer(
        examples["completion"],
        max_length=256,
        padding="max_length",
        truncation=True
    )["input_ids"]
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

tokenized_dataset = dataset.map(tokenize, batched=True)

# =============================
# Training args
# =============================
training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=3e-5,
    logging_steps=5,
    save_steps=50,
    fp16=torch.cuda.is_available(),
    save_total_limit=3,
    prediction_loss_only=True
)

# =============================
# Trainer
# =============================
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"] if "train" in tokenized_dataset else tokenized_dataset,
    tokenizer=tokenizer
)

# =============================
# Eƒüitim ba≈ülat
# =============================
trainer.train()

# Modeli kaydet
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print(f"‚úÖ Eƒüitim tamamlandƒ±, model kaydedildi: {output_dir}")
#    pip install transformers peft accelerate bitsandbytes datasets

##############################################################################
##############################################################################
##############################################################################
#SambaLingo-Turkish-Chat sohbet 
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_dir = "C:/huggingface/SambaLingo-Turkish-Chat-Finetuned"
device = "cuda" if torch.cuda.is_available() else "cpu"

# Model ve tokenizer y√ºkleme
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(model_dir).to(device)

# Test loop
while True:
    prompt = input("\nüß† Soru: ")
    if prompt.lower() in ["exit", "quit"]:
        break

    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output_ids = model.generate(
        **inputs,
        max_new_tokens=128,
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )

    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print(f"üí¨ Cevap: {response}")
##############################################################################
##############################################################################
##############################################################################




