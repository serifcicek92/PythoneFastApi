import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset

# -------------------
# 1ï¸âƒ£ Ayarlar
# -------------------
base_model = "redrussianarmy/gpt2-turkish-cased"
dataset_path = "kural.jsonl"
output_dir = r"C:\huggingface\gpt2-turkish-cased-finetuned"
epochs = 3
batch_size = 1
lr = 1e-4
num_workers = 0  # Windows stabilitesi

print("\n--- Ayarlar ---")
print(f"Model: {base_model}")
print(f"Dataset: {dataset_path}")
print(f"Ã‡Ä±kÄ±ÅŸ klasÃ¶rÃ¼: {output_dir}")
print(f"Epochs: {epochs}")
print(f"Batch Size: {batch_size}")
print(f"Learning Rate: {lr}")
print(f"Num workers: {num_workers}")
print("----------------\n")

# -------------------
# 2ï¸âƒ£ Model ve Tokenizer
# -------------------
print("ğŸ“¥ Model ve tokenizer yÃ¼kleniyor...")
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(base_model)

# ğŸ”§ GPT2 modelleri pad_token iÃ§ermez â†’ ekliyoruz:
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print(f"Device: {device}")

# -------------------
# 3ï¸âƒ£ Dataset yÃ¼kleme ve tokenize
# -------------------
print("ğŸ“‚ Dataset yÃ¼kleniyor...")
dataset = load_dataset("json", data_files=dataset_path, split="train")

def tokenize_fn(example):
    text = example["prompt"] + " " + example["completion"]
    tokenized = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_fn)

# -------------------
# 4ï¸âƒ£ TrainingArguments ve Trainer
# -------------------
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    learning_rate=lr,
    logging_steps=2,
    save_strategy="epoch",
    report_to="none",
    gradient_accumulation_steps=2,  # kÃ¼Ã§Ã¼k batch iÃ§in GPU doluluÄŸu artÄ±rÄ±r
    fp16=torch.cuda.is_available()
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

# -------------------
# 5ï¸âƒ£ EÄŸitim
# -------------------
print("ğŸš€ EÄŸitim baÅŸlÄ±yor...")
trainer.train()

# -------------------
# 6ï¸âƒ£ Kaydet
# -------------------
print("ğŸ’¾ Model kaydediliyor...")
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"\nâœ… Fine-tune tamamlandÄ±! Model '{output_dir}' klasÃ¶rÃ¼ne kaydedildi.")
