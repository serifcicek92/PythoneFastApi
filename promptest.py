import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ================================
# 1ï¸âƒ£ Model ve tokenizer tanÄ±mÄ±
# ================================
base_model = "vngrs-ai/Kumru-2B"

model_cache_dir = os.path.join(os.getcwd(), "model_cache")

print("â³ Tokenizer yÃ¼kleniyor / indiriliyor...")
tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=model_cache_dir)

print("â³ Model yÃ¼kleniyor / indiriliyor...")
model = AutoModelForCausalLM.from_pretrained(base_model, cache_dir=model_cache_dir)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
print(f"âœ… {base_model} modeli yÃ¼klendi ({device})")

# ================================
# 2ï¸âƒ£ Basit prompt testi
# ================================
def generate_text(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    # token_type_ids varsa kaldÄ±r
    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

test_prompt = "Merhaba! Kendini tanÄ±tÄ±r mÄ±sÄ±n?"
answer = generate_text(test_prompt)
print(f"\nğŸ’¬ Test prompt: {test_prompt}")
print(f"ğŸ¤– Model cevabÄ±: {answer}")

# ================================
# 3ï¸âƒ£ Sohbet dÃ¶ngÃ¼sÃ¼
# ================================
print("\nğŸ’¬ Kumru-2B ile sohbet! (Ã§Ä±kmak iÃ§in 'q' yaz)")
while True:
    user_input = input("\nğŸ§  Sen: ")
    if user_input.lower() in ["q", "quit", "exit"]:
        print("ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
        break
    response = generate_text(user_input, max_new_tokens=256)
    print(f"ğŸ¤– Model: {response}")
