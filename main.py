from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import os
import psycopg2
import json
import re
import ollama
import requests  # Swagger API Ã§aÄŸrÄ±sÄ± iÃ§in
import time
from llama_cpp import Llama

app = FastAPI(title="Boyut Bilgisayar Kurumsal Asistan")

#burasÄ± llama kkullanÄ±lacaksa aÃ§Ä±lacak ramim yetmedi cpu Ã§alÄ±ÅŸÄ±yor gpu sorunlu
# print("ğŸ§  Model yÃ¼kleniyor... (birkaÃ§ saniye sÃ¼rebilir)")
# llm = Llama(
#     model_path="C:\\llama_cpp\\models\\Qwen3-14B-Q4_K_M.gguf",
#     n_gpu_layers=20,   # GPUâ€™ya kaÃ§ katman yÃ¼klenecek
#     n_ctx=4096,        # maksimum context (token penceresi)
#     n_threads=6,       # iÅŸlemci Ã§ekirdek sayÄ±sÄ±
#     verbose=True       # log detaylarÄ±nÄ± gÃ¶ster
# )
# print("âœ… Model yÃ¼klendi!")

print("ğŸ§  Model yÃ¼kleniyor... (birkaÃ§ saniye sÃ¼rebilir)")
llm = Llama(
    model_path="C:\\huggingface\\generated\\gpt2-turkish\\gpt2-turkish.gguf",  # kendi modelin
    n_ctx=2048,        # GPT-2 iÃ§in genelde yeterli
    n_threads=6,        # CPU Ã§ekirdek sayÄ±n kadar
    n_gpu_layers=0,     # GPU devre dÄ±ÅŸÄ±, CPU kullanÄ±lÄ±yor
    verbose=False
)
print("âœ… GPT-2 TÃ¼rkÃ§e model yÃ¼klendi!")


# ------------------------------
# PostgreSQL baÄŸlantÄ±sÄ±
# ------------------------------
DB_HOST = "localhost"
DB_NAME = "postgres"
DB_USER = "postgres"
DB_PASSWORD = "sql123"
DB_PORT = 5432

conn = psycopg2.connect(
    host=DB_HOST,
    dbname=DB_NAME,
    user=DB_USER,
    password=DB_PASSWORD,
    port=DB_PORT
)
cur = conn.cursor()

# ------------------------------
# Embedding modeli
# ------------------------------
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# ------------------------------
# Veri modelleri
# ------------------------------
class Endpoint(BaseModel):
    module: str
    service: str
    method: str
    endpoint: str
    description: str
    parameters: list
    elementtypeid: int
    menuid: int

class Question(BaseModel):
    user_id: str
    query: str

# ------------------------------
# Conversation history fonksiyonlarÄ±
# ------------------------------
def save_conversation(user_id, role, message_text):
    try:
        cur.execute("""
            INSERT INTO conversation_history (user_id, message_role, message_text)
            VALUES (%s, %s, %s)
        """, (user_id, role, message_text))
        conn.commit()
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))

def get_conversation_history(user_id, limit=3):
    try:
        cur.execute("""
            SELECT message_role, message_text
            FROM conversation_history
            WHERE user_id=%s AND 
                message_text IS NOT NULL AND 
                message_text <> '' AND
                created_at >= NOW() - interval '30 seconds' 
            ORDER BY created_at ASC
            LIMIT %s
        """, (user_id, limit))
        rows = cur.fetchall()
        #return [{"role": r[0], "content": r[1]} for r in reversed(rows)]
        return [{"role": r[0], "content": r[1]} for r in rows]
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))
# ------------------------------
# Endpoint ekleme
# ------------------------------
@app.post("/add_endpoint")
def add_endpoint(data: Endpoint):
    try:
        embedding = embedding_model.encode(data.description)
        
        embedding_str = "[" + ",".join(map(str, embedding)) + "]"

        cur.execute("""
            INSERT INTO api_endpoints (module, service, method, endpoint, description, parameters, embedding,elementtypeid,menuid)
            VALUES (%s, %s, %s, %s, %s, %s, %s::vector,%s,%s)
        """, (
            data.module,
            data.service,
            data.method,
            data.endpoint,
            data.description,
            data.parameters,
            embedding_str,
            data.elementtypeid,
            data.menuid
        ))
        conn.commit()
        return {"status": "success", "endpoint": data.endpoint}
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))
    
# VarsayÄ±msal Python Kodu (FastAPI/Psycopg2 kullanÄ±larak)
def add_business_rule(rule_text: str):
    try:
        # 1. Metni VektÃ¶rleÅŸtirme
        embedding = embedding_model.encode(rule_text)
        embedding_str = "[" + ",".join(map(str, embedding)) + "]"
        
        # 2. VeritabanÄ±na Kaydetme
        cur.execute("""
            INSERT INTO business_rules (rule_text, embedding)
            VALUES (%s, %s::vector)
        """, (rule_text, embedding_str))
        conn.commit()
        
        return {"status": "success", "eklenenbilgi": rule_text}
    except Exception as e:
        conn.rollback()
        raise HTTPException(status_code=500, detail=str(e))
# ------------------------------
# KullanÄ±cÄ± sorusu
# ------------------------------
@app.post("/ask")
def ask_question(data: Question):
    baslangic = time.time()
    print(f"ask-a girdi:{baslangic}")
    try:
        # 0ï¸âƒ£ YÃ¶netim Komutu KontrolÃ¼ (Tamamen DoÄŸru)
        if data.query.lower().startswith("bilgiekle:"):
            query_text = data.query[len("bilgiekle:"):].strip()
            if not query_text:
                return {"status": "error", "response": "Hata: 'bilgiekle:' komutundan sonra eklenecek bir metin girilmelidir."}
            return add_business_rule(query_text) # add_business_rule baÅŸarÄ±lÄ±/hatalÄ± JSON dÃ¶nmeli

        # 1ï¸âƒ£ KullanÄ±cÄ± mesajÄ±nÄ± DB'ye kaydet
        save_conversation(data.user_id, "user", data.query)

        # 2ï¸âƒ£ GeÃ§miÅŸ mesajlarÄ± al
        history = get_conversation_history(data.user_id, limit=1)

        # 3ï¸âƒ£ VektÃ¶r HazÄ±rlÄ±ÄŸÄ±
        query_embedding = embedding_model.encode(data.query)
        embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

        # 4ï¸âƒ£ RAG: Endpoint ve Ä°ÅŸ KuralÄ± AramasÄ±
        cur.execute("""
                SELECT 
                    ae.module, ae.service, ae.method, ae.endpoint,
                    ae.description, ae.parameters,
                    ae.embedding <#> %s::vector AS dist,
                    ae.elementtypeid AS "elementtype"
                FROM api_endpoints ae
                WHERE ae.elementtypeid IN (
                    SELECT id FROM elementtype WHERE aktif=1
                )
                ORDER BY dist ASC
                LIMIT 4
            """, (embedding_str,))
        endpoint_matches = cur.fetchall()

        # b) Ä°ÅŸ KuralÄ± AramasÄ± (Sadece kural metnini al)
        cur.execute("""
            SELECT rule_text,  
                   embedding <-> %s::vector AS dist
            FROM business_rules
            ORDER BY dist ASC
            LIMIT 2
        """, (embedding_str,))
        rule_matches = cur.fetchall()
        
        # 5ï¸âƒ£ Konteksleri Formatlama ve BirleÅŸtirme (Tek ve GÃ¼Ã§lÃ¼ SYSTEM Prompt'u Ä°Ã§in)

        # Endpoint Konteksi (Zengin Bilgi)
        endpoint_filtered = [m for m in endpoint_matches if m[6] ];#< 0.6
        endpoint_context = ""
        if not endpoint_filtered:
            endpoint_context = "KullanÄ±cÄ± sorusuyla alakalÄ± bir API uÃ§ noktasÄ± bulunamadÄ±."
        else:
            endpoint_context = "\n---\n".join([
                f"Endpoint: {m[3]} | Metod: {m[2]} | ModÃ¼l: {m[0]} | Elementtype: {m[7]}\nAÃ§Ä±klama: {m[4]}\nParametreler: {m[5]}"
                for m in endpoint_matches
            ]) or "KullanÄ±cÄ± sorusuyla alakalÄ± bir API uÃ§ noktasÄ± (endpoint) bulunamadÄ±."
        #print(f"endpointcontex : {endpoint_context}")
        
        # Ä°ÅŸ KuralÄ± Konteksi (DoÄŸrudan Kural Metinleri)
        #rule_context = "\n- ".join([m[0] for m in rule_matches])
        rule__filtered = [m for m in rule_matches if m[-1] < 0.4];
        rule_context = ""
        if rule__filtered:
            rule_context_list = [m[0] for m in rule_matches]
            
            if rule_context_list:
                # KayÄ±tlarÄ± numaralandÄ±rarak (KayÄ±t 1, KayÄ±t 2, ...) daha net bir yapÄ± oluÅŸturuyoruz
                for i, rule_text in enumerate(rule_context_list):
                    rule_context += f"KayÄ±t {i + 1}: {rule_text.strip()}\n" # Yeni satÄ±r ve KayÄ±t X: baÅŸlÄ±ÄŸÄ± eklenir
            else:
                rule_context = "KullanÄ±cÄ± sorusuyla alakalÄ± bir iÅŸ kuralÄ± bulunamadÄ±."


        # 6ï¸âƒ£ Tek, GÃ¼Ã§lÃ¼ ve DÃ¼zgÃ¼n JSON FormatÄ± Zorlayan System Prompt'u OluÅŸturma
        system_content = f"""
Sen Boyut Bilgisayar adÄ±na nervus programÄ± iÃ§in gÃ¶rev yapan bir yapay zeka asistanÄ±sÄ±n.

VereceÄŸin cevaplar her zaman {{"response":"YanÄ±tÄ±n mesaj kÄ±smÄ±.","system_debug":"sisteme vermek istediÄŸin mesaj varsa","bilgiiste":**true/false**,"endpoint":"...", "metod":"...","modul":"...","elementtype":"..."}} ÅŸeklinde yanlÄ±zca json formatÄ±nda olmalÄ±. Chat iÃ§in bir mesajÄ±n varsa **response** keyinin value kÄ±smÄ±na yaz.

EÄŸer kullanÄ±cÄ±nÄ±n sorduÄŸu soru aÅŸaÄŸÄ±da vereceÄŸim [REHBER] olarak belirttiÄŸim listedeki bir kayÄ±t ile eÅŸleÅŸirse onu Json iÃ§inde dÃ¶n bu kayÄ±tlardan referans al.

EÄŸer kullanÄ±cÄ± bir liste istiyorsa:
-[ENDPOINTLER] olarak belirttiÄŸim listede bir endpoint varsa endpointi hiÃ§birÅŸekilde deÄŸiÅŸtirmeden jsondaki endpoint keyine value olarak yaz.Kesinlike endpointi BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf veya kelime deÄŸiÅŸtirme olduÄŸu gibi kalsÄ±n. MesajÄ±n varsa response keyine olduÄŸu gibi yaz. Elementtype kÄ±smÄ±nÄ±da elementtype keyine olduÄŸu gibi yaz deÄŸiÅŸtirme
-Ben senin bu gÃ¶nderdiÄŸin endpoint ile data Ã§ekip gÃ¶stereceÄŸim.
-EÄŸer endpoint var ise bilgiiste keyi true olacak.

--- [ENDPOINTLER]  ---
{endpoint_context}

--- [REHBER] ---
{rule_context}
"""
        system_message = {"role": "system", "content": system_content}
        
        # 7ï¸âƒ£ TÃ¼m mesajlarÄ± birleÅŸtir (SADECE TEK SYSTEM MESAJI + GEÃ‡MÄ°Å)
        messages = [system_message] + history 
        #return endpoint_context;
        #print(f"ollamaya gÃ¶nderecek mesaj:{messages}")
        bitis = time.time()
        gecen_sure = bitis - baslangic
        print(f"GeÃ§en sÃ¼re: {gecen_sure:.3f} saniye ({gecen_sure*1000:.0f} milisaniye)")
        print("-----------------------------------")
        print(f"Endpointler:{endpoint_context}")
        print(f"Rehber:{rule_context}")
        print("---------------------------------------------------------")
        print(f"HÄ°STORY:{history}")
        ("---------------------------------------------------------")
        baslangic = time.time()
        
        # 8ï¸âƒ£ Ollama GPT-OSS Ã§aÄŸrÄ±sÄ±
        response = ollama.chat(
            #model='gpt-oss:latest',
            #model='llama3.1:latest',
            #model='deepseek-r1:14b',
            #model='mistral:7b',
            #model='llama3.1:70b-instruct-q2_K',
            #model='gemma3:12b',
            #model='phi3:14b',
            #model='llava:13b', #yanlÄ±ÅŸ cevap Ã¼retiyor
            model = 'qwen2.5:1.5b',
            #model = 'phi3:3.8b',
            messages=messages
        )
        bitis = time.time()
        gecen_sure = bitis - baslangic
        print(f"Model GeÃ§en sÃ¼re: {gecen_sure:.3f} saniye ({gecen_sure*1000:.0f} milisaniye)")
        print("--------------------------------")
        print(f"ollamadan dÃ¶nen:{response}")
        print("---------------------------------------------------------")
        
        assistant_text = response["message"]["content"]
        
        # 9ï¸âƒ£ YanÄ±tÄ± DB'ye kaydet (API Ã§aÄŸrÄ±sÄ± ve JSON iÅŸlemeyi kaldÄ±rdÄ±m, sadece yanÄ±tÄ± dÃ¶ndÃ¼rdÃ¼m)
        # Bu kÄ±sÄ±m daha sonra Ajan mantÄ±ÄŸÄ± olarak ayrÄ± bir fonksiyonda ele alÄ±nmalÄ±dÄ±r.
        save_conversation(data.user_id, "assistant", assistant_text)

        return {"answer": assistant_text}
    
        ##buradan aÅŸaÄŸÄ±sÄ± ÅŸuan iÅŸlemiyor ama endpoint varsa bilgi Ã§ek gibi dÃ¼ÅŸÃ¼ndÃ¼m
        api_data = None
        try:
            parsed = json.loads(assistant_text)
            if parsed.get("endpointlist"):
                # API URL, senin verdiÄŸin base URL + endpoint
                base_url = "http://192.168.3.101:6969"
                endpoint_list = parsed.get("endpointlist", [])
                if isinstance(endpoint_list, list):
                    # sadece ilk endpointi al
                    endpoint_path = endpoint_list[0]
                else:
                    endpoint_path = endpoint_list
                print("--------------------------------------")
                print(f"endpoint:{endpoint_path}")
                print("--------------------------------------")
                endpoint_url = f"{base_url}{endpoint_path}"
                print(f"endpoint_url:{endpoint_url}")
                print("--------------------------------------")
                # DBâ€™den endpoint parametrelerini al
                cur.execute("""
                    SELECT parameters
                    FROM api_endpoints
                    WHERE endpoint = %s
                """, (endpoint_url,))
                
                

                params = {
                            "FaturaTarihiBas": "2025-09-01",
                            "FaturaTarihiBit": "2025-10-28"
                        }
                # API key header
                API_KEY="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1laWQiOiIxIiwidW5pcXVlX25hbWUiOiJhZG1pbiIsInJvbGUiOiJbXSIsIlN1YmVJZCI6IjEiLCJDZXBTdWJlSWQiOiIyODEiLCJuYmYiOjE3NjA0NDk0MzYsImV4cCI6MTc2MzEyNzgzNiwiaWF0IjoxNzYwNDQ5NDM2LCJpc3MiOiJCb3l1dCIsImF1ZCI6IkJveXV0In0.IpS9v-soj6ea62tXGL40NvwbV9j3Suh1KCYHBvbiey0"
                headers = {
                    "Authorization": f"Bearer {API_KEY}",
                    "Content-Type": "application/json"
                }
                # API Ã§aÄŸrÄ±sÄ±
                try:
                    api_response = requests.get(endpoint_url, params=params, headers=headers)
                    try:
                        api_data = api_response.json()
                    except ValueError:
                        api_data = {"raw_response": api_response.text}
                except Exception as api_e:
                    print("API Ã§aÄŸrÄ±sÄ± sÄ±rasÄ±nda hata:", api_e)
                    api_data = {"error": str(api_e)}
                # DÃ¶nen veriyi assistant_text iÃ§ine ekle
                api_data = api_response.json()
                return api_data
                assistant_text = json.dumps(parsed, ensure_ascii=False)
                return {"answer": assistant_text}
        except Exception as e:
            # Hata olsa bile assistant_textâ€™i dÃ¶ndÃ¼r
            print("API Ã§aÄŸrÄ±sÄ± sÄ±rasÄ±nda hata:", e)
            return e



        # 8ï¸âƒ£ Endpoint Ã§aÄŸrÄ±sÄ± gerekiyorsa
        try:
            parsed = json.loads(assistant_text)
            if parsed.get("endpointvarsa"):
                endpoint_url = parsed["endpointvarsa"]
                params = parsed.get("parameters", {})
                api_response = requests.get(endpoint_url, params=params)
                parsed["resonse"] = api_response.json()
                assistant_text = json.dumps(parsed, ensure_ascii=False)
        except Exception:
            pass

        # 9ï¸âƒ£ YanÄ±tÄ± DB'ye kaydet
        save_conversation(data.user_id, "assistant", assistant_text)

        return {"answer": assistant_text}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


#Ä°ÅŸ sÃ¼reci bilgi tabanÄ± referansÄ±ndan yararlanÄ±rken kullanÄ±cÄ±nÄ±n cevabÄ±na en uygun kayÄ±ttan cevap oluÅŸturup ver
    





# ------------------------------
# Eski embeddingleri gÃ¼ncelle
# ------------------------------
def update_embeddings(table_name: str, text_column: str, id_column: str = "id"):
    cur.execute(f"SELECT {id_column}, {text_column} FROM {table_name}")
    rows = cur.fetchall()
    
    for row in rows:
        record_id, text = row
        if not text.strip():
            continue
        embedding = embedding_model.encode(text)
        embedding_100 = embedding[:100]  # Cube max 100 boyut
        vec_str = "(" + ",".join(map(str, embedding_100)) + ")"
        
        cur.execute(f"""
            UPDATE {table_name}
            SET embedding = CUBE(%s)
            WHERE {id_column} = %s
        """, (vec_str, record_id))
    
    conn.commit()
    print(f"{table_name} tablosundaki embeddingler gÃ¼ncellendi.")


@app.post("/ollamaask")
def ollama_ask(data: Question):
    response = ollama.chat(
            #model='gpt-oss:latest',
            #model='llama3.1:latest',
            #model='deepseek-r1:14b',
            #model='mistral:7b',
            #model='llama3.1:70b-instruct-q2_K',
            #model='gemma3:12b',
            #model='phi3:14b', #yavaÅŸ
            model='llava:13b',
            messages=[{'role': 'user', 'content': data.query}]
        )
    assistant_text = response["message"]["content"]
    return {"answer": assistant_text}








# 8ï¸âƒ£ KoboldCPP GPT Ã§aÄŸrÄ±sÄ±
def query_koboldcpp(messages):
    url = "http://127.0.0.1:5001/api/v1/generate"  # KoboldCPP varsayÄ±lan API
    prompt = ""
    for msg in messages:
        prompt += f"{msg['role'].upper()}: {msg['content']}\n"
    prompt += "ASSISTANT:"

    payload = {
        "prompt": prompt,
        "max_context_length": 2048,
        "max_length": 512,
        "temperature": 0.7,
        "stop_sequence": ["USER:", "SYSTEM:"],
        "stream": False
    }

    response = requests.post(url, json=payload)
    if response.status_code != 200:
        raise HTTPException(status_code=500, detail="KoboldCPP API hatasÄ±")
    data = response.json()
    return data.get("results", [{}])[0].get("text", "")







# ------------------------------
# KullanÄ±cÄ± sorusu llama 
# ------------------------------
@app.post("/askllama_cpp_local")
def ask_question_local(data: Question):
    baslangic = time.time()
    print(f"ask-a girdi:{baslangic}")
    try:
        # 0ï¸âƒ£ YÃ¶netim Komutu KontrolÃ¼ (Tamamen DoÄŸru)
        if data.query.lower().startswith("bilgiekle:"):
            query_text = data.query[len("bilgiekle:"):].strip()
            if not query_text:
                return {"status": "error", "response": "Hata: 'bilgiekle:' komutundan sonra eklenecek bir metin girilmelidir."}
            return add_business_rule(query_text) # add_business_rule baÅŸarÄ±lÄ±/hatalÄ± JSON dÃ¶nmeli

        # 1ï¸âƒ£ KullanÄ±cÄ± mesajÄ±nÄ± DB'ye kaydet
        save_conversation(data.user_id, "user", data.query)

        # 2ï¸âƒ£ GeÃ§miÅŸ mesajlarÄ± al
        history = get_conversation_history(data.user_id, limit=1)

        # 3ï¸âƒ£ VektÃ¶r HazÄ±rlÄ±ÄŸÄ±
        query_embedding = embedding_model.encode(data.query)
        query_embedding_100 = query_embedding[:100]#384 olabilir ama hata verir
        vec_str = "(" + ",".join(map(str, query_embedding_100)) + ")"

        # 4ï¸âƒ£ RAG: Endpoint ve Ä°ÅŸ KuralÄ± AramasÄ±

        # a) Endpoint AramasÄ± (AÃ§Ä±klama, Parametre ve Endpoint'i al)
        cur.execute("""
            SELECT ae.module, ae.service, ae.method, ae.endpoint, ae.description, 
                   ae.parameters, cube_distance(ae.embedding, CUBE(%s)) AS dist,
                   ae.elementtypeid as "elementtype"
            FROM api_endpoints ae
            left join elementtype e on e.id = ae.elementtypeid  and e.aktif=1
            ORDER BY dist ASC
            LIMIT 2
        """, (vec_str,))
        endpoint_matches = cur.fetchall()

        # b) Ä°ÅŸ KuralÄ± AramasÄ± (Sadece kural metnini al)
        cur.execute("""
            SELECT rule_text,  cube_distance(embedding, CUBE(%s)) as dist
            FROM business_rules
            ORDER BY dist ASC
            LIMIT 2
        """, (vec_str,))
        rule_matches = cur.fetchall()
        
        # 5ï¸âƒ£ Konteksleri Formatlama ve BirleÅŸtirme (Tek ve GÃ¼Ã§lÃ¼ SYSTEM Prompt'u Ä°Ã§in)

        # Endpoint Konteksi (Zengin Bilgi)
        endpoint_filtered = [m for m in endpoint_matches if m[-1] ];#< 0.6
        endpoint_context = ""
        if not endpoint_filtered:
            endpoint_context = "KullanÄ±cÄ± sorusuyla alakalÄ± bir API uÃ§ noktasÄ± bulunamadÄ±."
        else:
            endpoint_context = "\n---\n".join([
                f"Endpoint: {m[3]} | Metod: {m[2]} | ModÃ¼l: {m[0]} | Elementtype: {m[7]}\nAÃ§Ä±klama: {m[4]}\nParametreler: {m[5]}"
                for m in endpoint_matches
            ]) or "KullanÄ±cÄ± sorusuyla alakalÄ± bir API uÃ§ noktasÄ± (endpoint) bulunamadÄ±."
        #print(f"endpointcontex : {endpoint_context}")
        
        # Ä°ÅŸ KuralÄ± Konteksi (DoÄŸrudan Kural Metinleri)
        #rule_context = "\n- ".join([m[0] for m in rule_matches])
        rule__filtered = [m for m in rule_matches if m[-1] < 0.4];
        rule_context = ""
        if rule__filtered:
            rule_context_list = [m[0] for m in rule_matches]
            
            if rule_context_list:
                # KayÄ±tlarÄ± numaralandÄ±rarak (KayÄ±t 1, KayÄ±t 2, ...) daha net bir yapÄ± oluÅŸturuyoruz
                for i, rule_text in enumerate(rule_context_list):
                    rule_context += f"KayÄ±t {i + 1}: {rule_text.strip()}\n" # Yeni satÄ±r ve KayÄ±t X: baÅŸlÄ±ÄŸÄ± eklenir
            else:
                rule_context = "KullanÄ±cÄ± sorusuyla alakalÄ± bir iÅŸ kuralÄ± bulunamadÄ±."


        # 6ï¸âƒ£ Tek, GÃ¼Ã§lÃ¼ ve DÃ¼zgÃ¼n JSON FormatÄ± Zorlayan System Prompt'u OluÅŸturma
        system_content = f"""
Sen Boyut Bilgisayar adÄ±na nervus programÄ± iÃ§in gÃ¶rev yapan bir yapay zeka asistanÄ±sÄ±n.

VereceÄŸin cevaplar her zaman {{"response":"YanÄ±tÄ±n mesaj kÄ±smÄ±.","system_debug":"sisteme vermek istediÄŸin mesaj varsa","bilgiiste":**true/false**,"endpoint":"...", "metod":"...","modul":"...","elementtype":"..."}} ÅŸeklinde yanlÄ±zca json formatÄ±nda olmalÄ±. Chat iÃ§in bir mesajÄ±n varsa **response** keyinin value kÄ±smÄ±na yaz.

EÄŸer kullanÄ±cÄ±nÄ±n sorduÄŸu soru aÅŸaÄŸÄ±da vereceÄŸim [REHBER] olarak belirttiÄŸim listedeki bir kayÄ±t ile eÅŸleÅŸirse onu Json iÃ§inde dÃ¶n bu kayÄ±tlardan referans al.

EÄŸer kullanÄ±cÄ± bir liste istiyorsa:
-[ENDPOINTLER] olarak belirttiÄŸim listede bir endpoint varsa endpointi hiÃ§birÅŸekilde deÄŸiÅŸtirmeden jsondaki endpoint keyine value olarak yaz.Kesinlike endpointi BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf veya kelime deÄŸiÅŸtirme olduÄŸu gibi kalsÄ±n. MesajÄ±n varsa response keyine olduÄŸu gibi yaz. Elementtype kÄ±smÄ±nÄ±da elementtype keyine olduÄŸu gibi yaz deÄŸiÅŸtirme
-Ben senin bu gÃ¶nderdiÄŸin endpoint ile data Ã§ekip gÃ¶stereceÄŸim.
-EÄŸer endpoint var ise bilgiiste keyi true olacak.

--- [ENDPOINTLER]  ---
{endpoint_context}

--- [REHBER] ---
{rule_context}
"""
        system_message = {"role": "system", "content": system_content}
        
        # 7ï¸âƒ£ TÃ¼m mesajlarÄ± birleÅŸtir (SADECE TEK SYSTEM MESAJI + GEÃ‡MÄ°Å)
        messages = [system_message] + history 

        #print(f"ollamaya gÃ¶nderecek mesaj:{messages}")
        bitis = time.time()
        gecen_sure = bitis - baslangic
        print(f"GeÃ§en sÃ¼re: {gecen_sure:.3f} saniye ({gecen_sure*1000:.0f} milisaniye)")
        print("-----------------------------------")
        print(f"Endpointler:{endpoint_context}")
        print(f"Roller:{rule_context}")
        print("---------------------------------------------------------")
        print(f"HÄ°STORY:{history}")
        ("---------------------------------------------------------")
        baslangic = time.time()
        response = llm.create_chat_completion(
                messages=messages,
                max_tokens=200,
                temperature=0.7,
                top_p=0.9
            )
        bitis = time.time()
        gecen_sure = bitis - baslangic
        print(f"Model GeÃ§en sÃ¼re: {gecen_sure:.3f} saniye ({gecen_sure*1000:.0f} milisaniye)")
        print("--------------------------------")
        print(f"ollamadan dÃ¶nen:{response}")
        print("---------------------------------------------------------")
        assistant_raw = response["choices"][0]["message"]["content"]

        cleaned_text = re.sub(r"<think>.*?</think>", "", assistant_raw, flags=re.DOTALL).strip()
        
        try:
            assistant_text = json.loads(cleaned_text)
        except json.JSONDecodeError:
            assistant_text = {"response": cleaned_text}

        #assistant_text = response#response["choices"][0]["message"]["content"]
        
        # 9ï¸âƒ£ YanÄ±tÄ± DB'ye kaydet (API Ã§aÄŸrÄ±sÄ± ve JSON iÅŸlemeyi kaldÄ±rdÄ±m, sadece yanÄ±tÄ± dÃ¶ndÃ¼rdÃ¼m)
        # Bu kÄ±sÄ±m daha sonra Ajan mantÄ±ÄŸÄ± olarak ayrÄ± bir fonksiyonda ele alÄ±nmalÄ±dÄ±r.
        #save_conversation(data.user_id, "assistant", assistant_text)

        return {"answer": assistant_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ------------------------------
# KullanÄ±cÄ± sorusu llama 
# ------------------------------
@app.post("/askllama_cpp")
def ask_question_llama_cpp(data: Question):
    import requests, json, time

    
    baslangic = time.time()
    print(f"askllama_cpp Ã§aÄŸrÄ±ldÄ±: {baslangic}")

    try:
        # 1ï¸âƒ£ YÃ¶netim komutu kontrolÃ¼
        if data.query.lower().startswith("bilgiekle:"):
            query_text = data.query[len("bilgiekle:"):].strip()
            if not query_text:
                return {"status": "error", "response": "Hata: 'bilgiekle:' komutundan sonra eklenecek bir metin girilmelidir."}
            return add_business_rule(query_text)

        # 2ï¸âƒ£ Mesaj kaydet
        save_conversation(data.user_id, "user", data.query)

        # 3ï¸âƒ£ GeÃ§miÅŸ konuÅŸmayÄ± al
        history = get_conversation_history(data.user_id, limit=1)

        # 4ï¸âƒ£ Embedding hesapla
        query_embedding = embedding_model.encode(data.query)
        embedding_str = "[" + ",".join(map(str, query_embedding)) + "]"

        # 5ï¸âƒ£ API endpoint arama
        cur.execute("""
                SELECT 
                    ae.module, ae.service, ae.method, ae.endpoint,
                    ae.description, ae.parameters,
                    ae.embedding <#> %s::vector AS dist,
                    ae.elementtypeid AS "elementtype"
                FROM api_endpoints ae
                WHERE ae.elementtypeid IN (
                    SELECT id FROM elementtype WHERE aktif=1
                )
                ORDER BY dist ASC
                LIMIT 4
            """, (embedding_str,))
        endpoint_matches = cur.fetchall()

        # 6ï¸âƒ£ Ä°ÅŸ kuralÄ± arama
        cur.execute("""
            SELECT rule_text,  
                   embedding <-> %s::vector AS dist
            FROM business_rules
            ORDER BY dist ASC
            LIMIT 2
        """, (embedding_str,))
        rule_matches = cur.fetchall()

        # Endpoint context
        endpoint_filtered = [m for m in endpoint_matches if m[6]]
        endpoint_context = "\n---\n".join([
            f"Endpoint: {m[3]} | Metod: {m[2]} | ModÃ¼l: {m[0]} | Elementtype: {m[7]}\nAÃ§Ä±klama: {m[4]}\nParametreler: {m[5]}"
            for m in endpoint_filtered
        ]) or "KullanÄ±cÄ± sorusuyla alakalÄ± bir API uÃ§ noktasÄ± bulunamadÄ±."

        # Rule context
        rule_filtered = [m for m in rule_matches if m[-1] < 0.4]
        rule_context = ""
        if rule_filtered:
            for i, r in enumerate(rule_filtered):
                rule_context += f"KayÄ±t {i + 1}: {r[0].strip()}\n"
        else:
            rule_context = "KullanÄ±cÄ± sorusuyla alakalÄ± bir iÅŸ kuralÄ± bulunamadÄ±."

        # System mesajÄ±
        system_content = f"""
Sen Boyut Bilgisayar adÄ±na nervus programÄ± iÃ§in gÃ¶rev yapan bir yapay zeka asistanÄ±sÄ±n.

VereceÄŸin cevaplar her zaman {{"response":"YanÄ±tÄ±n mesaj kÄ±smÄ±.","system_debug":"sisteme vermek istediÄŸin mesaj varsa","bilgiiste":true/false,"endpoint":"...","metod":"...","modul":"...","elementtype":"..."}} formatÄ±nda olmalÄ±.

--- [ENDPOINTLER]  ---
{endpoint_context}

--- [REHBER] ---
{rule_context}
"""
        system_message = {"role": "system", "content": system_content}

        # GeÃ§miÅŸ mesajlarÄ± birleÅŸtir
        # messages = [system_message] + history + [{"role": "user", "content": data.query}]
        messages = [system_message] + [{"role": "user", "content": data.query}]


        # 7ï¸âƒ£ Prompt hazÄ±rlÄ±ÄŸÄ±
        prompt = ""
        # for msg in messages:
        #     prompt += f"{msg['role'].upper()}: {msg['content']}\n"
        # prompt += "ASSISTANT:"

        prompt = f"{data.query}"

        
        # 8ï¸âƒ£ llama.cpp REST API Ã§aÄŸrÄ±sÄ±
        url = "http://192.168.3.32:8081/completion"
        payload = {
            "prompt": prompt,
            "n_predict": 128,  # KÃ¼Ã§Ã¼k model iÃ§in yeterli
            "temperature": 0.7,
            "stop": ["\n"],  # Tek satÄ±r output
            "stream": False
        }

        response = requests.post(url, json=payload)
        if response.status_code != 200:
            raise Exception(f"Llama.cpp HTTP HatasÄ±: {response.status_code} - {response.text}")

        result = response.json()
        return result
        assistant_text = result.get("content", "").strip()

        # 9ï¸âƒ£ YanÄ±tÄ± DB'ye kaydet
        save_conversation(data.user_id, "assistant", assistant_text)

        return {"answer": assistant_text}

    except Exception as e:
        print("askllama_cpp hata:", e)
        raise HTTPException(status_code=500, detail=str(e))


def query_llamacpp(messages):
    url = "http://127.0.0.1:8081/completion"  # llama.cpp server endpoint
    
    # MesajlarÄ± Llama'ya uygun formatta birleÅŸtiriyoruz
    prompt = ""
    for msg in messages:
        role = msg["role"].upper()
        content = msg["content"].strip()
        prompt += f"{role}: {content}\n"
    prompt += "ASSISTANT:"

    payload = {
        "prompt": prompt,
        "n_predict": 256,           # 512 bÃ¼yÃ¼kse bazen timeout olabilir, 256 daha gÃ¼venli
        "temperature": 0.7,
        "top_p": 0.9,
        "stop": ["USER:", "SYSTEM:", "ASSISTANT:"],
        "stream": False
    }

    try:
        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()
        data = response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"llama.cpp API hatasÄ±: {e}")

    # BazÄ± sÃ¼rÃ¼mler 'content' deÄŸil 'content'/'response'/'text' anahtarÄ± dÃ¶ndÃ¼rebiliyor:
    return data.get("content") or data.get("response") or data.get("text") or ""

## servis olarak baÅŸlatma    llama-cli.exe --model gpt2-turkish.gguf --server --port 8080
##llama-cli.exe --model C:\huggingface\generated\gpt2-turkish\gpt2-turkish.gguf --server 0.0.0.0 --port 8080
## yada llama-cli.exe --model gpt2-turkish.gguf --server --port 8080 --n-predict 200 --temperature 0.7 --top-p 0.9
##DoÄŸrusu
##C:\llama_cpp\llama-server.exe --model "C:\huggingface\generated\gpt2-turkish\gpt2-turkish.gguf" --port 8081 --host 0.0.0.0
#Chat Åeklinde
#C:\llama_cpp>C:\llama_cpp\llama-server.exe --model "C:\huggingface\generated\gpt2-turkish\gpt2-turkish.gguf" --port 8081 --host 0.0.0.0 --chat


## tetiklemesi response = query_llamacpp(messages)
